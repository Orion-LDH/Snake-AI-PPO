
# Snake Reinforcement Learning with PPO and ICM

[English](#english) | [ä¸­æ–‡](#ä¸­æ–‡)

<a name="english"></a>
## English

###  Project Overview
This project implements an advanced Reinforcement Learning agent combining Proximal Policy Optimization (PPO) with an Intrinsic Curiosity Module (ICM) to master the classic Snake game. The agent learns through a sophisticated combination of:

- **Extrinsic rewards**: Traditional game rewards (food collection, death penalty)
- **Intrinsic rewards**: Curiosity-driven exploration rewards generated by ICM
- **Enhanced state representation**: Comprehensive environment encoding

###  Key Features

####  Advanced RL Architecture
- **PPO Algorithm**:
  - Clipped objective function for stable policy updates
  - Generalized Advantage Estimation (GAE) for better credit assignment
  - Multi-epoch minibatch updates for sample efficiency
- **ICM Module**:
  - Separate forward and inverse dynamics models
  - Feature encoding network
  - Automatic intrinsic reward calculation
  - Adjustable curiosity weighting

#### ğŸ•¹ Game Environment
- Customizable grid size (default 10x10)
- Pixel-perfect Pygame rendering
- Frame rate control for training speed
- Comprehensive observation space:
  - 3-direction danger detection
  - Head and body position encoding
  - Food location relative to head
  - Wall distance measurements
  - 8-direction body presence detection

####  Training Infrastructure
- TensorBoard logging for real-time monitoring
- CSV training metrics export
- Periodic model checkpointing
- Best model auto-saving
- Training interruption handling

###  Requirements



#### Software
- Python 3
- PyTorch 1.8+
- Pygame 2.0+
- NumPy
- TensorBoard

###  Installation
```bash
pip install torch pygame numpy tensorboard
```



###  Usage

#### Training Command
```bash
python snake_ppo.py
```

#### Training Options (via code modification)
1. Open `snake_ppo.py`
2. Modify the `Cfg` class parameters:
   - Environment settings (grid size, cell size)
   - Training parameters (learning rate, batch size)
   - Reward structure
   - Network architecture

#### Testing Command
```bash
python snake_ppo.py --test path/to/model.pt
```

### âš™ï¸ Configuration Details

#### Core PPO Parameters
| Parameter | Default | Description |
|-----------|---------|-------------|
| `LR` | 1e-4 | Learning rate |
| `GAMMA` | 0.995 | Discount factor |
| `LAMBDA` | 0.95 | GAE parameter |
| `EPS_CLIP` | 0.2 | PPO clip range |
| `BATCH_SIZE` | 2048 | Experience buffer size |
| `MINI_BATCH` | 256 | Update batch size |
| `K_EPOCHS` | 8 | Optimization epochs |

#### ICM Parameters
| Parameter | Default | Description |
|-----------|---------|-------------|
| `ICM_LR` | 3e-4 | ICM learning rate |
| `ICM_BETA` | 0.2 | Forward/inverse loss balance |
| `ICM_ETA` | 0.1 | Intrinsic reward scale |
| `ICM_FEATURE_DIM` | 128 | Encoded feature dimension |

#### Reward Structure
| Reward Type | Value | Description |
|-------------|-------|-------------|
| `REWARD_FOOD` | +10.0 | Collecting food |
| `REWARD_DEATH` | -10.0 | Hitting wall/body |
| `REWARD_STEP` | 0.0 | Per-step penalty |
| `REWARD_SURVIVAL` | +0.1 | Survival bonus |

### ğŸ“‚ File Structure

```
snake_ppo/
â”œâ”€â”€ runs/                   # Training outputs
â”‚   â”œâ”€â”€ [timestamp]/        # Individual run
â”‚   â”‚   â”œâ”€â”€ events.out.tfevents  # TensorBoard logs
â”‚   â”‚   â”œâ”€â”€ log.csv         # Training metrics
â”‚   â”‚   â”œâ”€â”€ best.pt         # Best performing model
â”‚   â”‚   â””â”€â”€ ckpt_[ep].pt    # Periodic checkpoints
â””â”€â”€ snake_ppo.py        # Main training script
```

###  Performance Metrics

During training, the following metrics are tracked:

1. **Episode Reward**: Total reward per episode
2. **Score**: Snake length - 1 (foods collected)
3. **Episode Length**: Duration in steps
4. **Loss Components**:
   - Actor loss (policy gradient)
   - Critic loss (value estimation)
   - Entropy (exploration bonus)
   - ICM loss (curiosity module)
5. **Intrinsic Reward**: Average curiosity reward



### Customization Guide

#### Environment Modifications
```python
# In Cfg class:
GRID_W = 15  # Change grid width
GRID_H = 15  # Change grid height
CELL_SIZE = 30  # Change rendering size
```

#### Reward Adjustments
```python
REWARD_FOOD = 15.0  # Increase food reward
REWARD_DEATH = -15.0 # Stronger death penalty
REWARD_SURVIVAL = 0.05 # Reduce survival bonus
```

#### Network Architecture
```python
HIDDEN_DIM = 512  # Smaller network
# or
HIDDEN_DIM = 1024 # Larger network
```

#### Training Parameters
```python
BATCH_SIZE = 4096  # Larger batches
LR = 3e-4         # Faster learning
GAMMA = 0.99      # Shorter-term focus
```



<a name="ä¸­æ–‡"></a>
## ä¸­æ–‡

### é¡¹ç›®æ¦‚è¿°
æœ¬é¡¹ç›®å®ç°äº†ç»“åˆPPO(è¿‘ç«¯ç­–ç•¥ä¼˜åŒ–)å’ŒICM(å†…åœ¨å¥½å¥‡å¿ƒæ¨¡å—)çš„å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“æ¥æŒæ¡ç»å…¸è´ªåƒè›‡æ¸¸æˆã€‚æ™ºèƒ½ä½“é€šè¿‡ä»¥ä¸‹è¦ç´ è¿›è¡Œå­¦ä¹ :

- **å¤–éƒ¨å¥–åŠ±**: ä¼ ç»Ÿæ¸¸æˆå¥–åŠ±(åƒé£Ÿç‰©ã€æ­»äº¡æƒ©ç½š)
- **å†…åœ¨å¥–åŠ±**: ICMç”Ÿæˆçš„å¥½å¥‡å¿ƒé©±åŠ¨æ¢ç´¢å¥–åŠ±
- **å¢å¼ºçŠ¶æ€è¡¨ç¤º**: å…¨é¢çš„ç¯å¢ƒç¼–ç 

###  æ ¸å¿ƒç‰¹æ€§

####  RLæ¶æ„
- **PPOç®—æ³•**:
  - å‰ªåˆ‡ç›®æ ‡å‡½æ•°å®ç°ç¨³å®šç­–ç•¥æ›´æ–°
  - å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡(GAE)ä¼˜åŒ–ä¿¡ç”¨åˆ†é…
  - å¤šè½®å°æ‰¹é‡æ›´æ–°æé«˜æ ·æœ¬æ•ˆç‡
- **ICMæ¨¡å—**:
  - ç‹¬ç«‹çš„å‰å‘å’Œé€†å‘åŠ¨åŠ›å­¦æ¨¡å‹
  - ç‰¹å¾ç¼–ç ç½‘ç»œ
  - è‡ªåŠ¨å†…åœ¨å¥–åŠ±è®¡ç®—
  - å¯è°ƒèŠ‚å¥½å¥‡å¿ƒæƒé‡

####  æ¸¸æˆç¯å¢ƒ
- å¯è‡ªå®šä¹‰ç½‘æ ¼å¤§å°(é»˜è®¤10x10)
- ç²¾ç¡®åˆ°åƒç´ çš„Pygameæ¸²æŸ“
- å¸§ç‡æ§åˆ¶è°ƒèŠ‚è®­ç»ƒé€Ÿåº¦
- å…¨é¢çš„è§‚å¯Ÿç©ºé—´:
  - 3æ–¹å‘å±é™©æ£€æµ‹
  - è›‡å¤´å’Œèº«ä½“ä½ç½®ç¼–ç 
  - é£Ÿç‰©ç›¸å¯¹ä½ç½®
  - å¢™å£è·ç¦»æµ‹é‡
  - 8æ–¹å‘èº«ä½“å­˜åœ¨æ£€æµ‹

####  è®­ç»ƒåŸºç¡€è®¾æ–½
- TensorBoardå®æ—¶ç›‘æ§
- CSVè®­ç»ƒæŒ‡æ ‡å¯¼å‡º
- å®šæœŸæ¨¡å‹æ£€æŸ¥ç‚¹
- è‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡å‹
- è®­ç»ƒä¸­æ–­å¤„ç†

###  ç³»ç»Ÿè¦æ±‚



#### è½¯ä»¶
- Python 3
- PyTorch 1.8+
- Pygame 2.0+
- NumPy
- TensorBoard

###  å®‰è£…

```bash
pip install torch pygame numpy tensorboard
```

###  ä½¿ç”¨æŒ‡å—

#### è®­ç»ƒå‘½ä»¤
```bash
python snake_ppo.py
```

#### è®­ç»ƒé€‰é¡¹(é€šè¿‡ä»£ç ä¿®æ”¹)
1. æ‰“å¼€ `snake_ppo_icm.py`
2. ä¿®æ”¹ `Cfg` ç±»å‚æ•°:
   - ç¯å¢ƒè®¾ç½®(ç½‘æ ¼å¤§å°ã€å•å…ƒæ ¼å°ºå¯¸)
   - è®­ç»ƒå‚æ•°(å­¦ä¹ ç‡ã€æ‰¹é‡å¤§å°)
   - å¥–åŠ±ç»“æ„
   - ç½‘ç»œæ¶æ„

#### æµ‹è¯•å‘½ä»¤
```bash
python snake_ppo.py --test æ¨¡å‹è·¯å¾„.pt
```

###  é…ç½®è¯¦æƒ…

#### æ ¸å¿ƒPPOå‚æ•°
| å‚æ•° | é»˜è®¤å€¼ | æè¿° |
|------|--------|------|
| `LR` | 1e-4 | å­¦ä¹ ç‡ |
| `GAMMA` | 0.995 | æŠ˜æ‰£å› å­ |
| `LAMBDA` | 0.95 | GAEå‚æ•° |
| `EPS_CLIP` | 0.2 | PPOå‰ªåˆ‡èŒƒå›´ |
| `BATCH_SIZE` | 2048 | ç»éªŒç¼“å†²åŒºå¤§å° |
| `MINI_BATCH` | 256 | æ›´æ–°æ‰¹é‡å¤§å° |
| `K_EPOCHS` | 8 | ä¼˜åŒ–è½®æ•° |

#### ICMå‚æ•°
| å‚æ•° | é»˜è®¤å€¼ | æè¿° |
|------|--------|------|
| `ICM_LR` | 3e-4 | ICMå­¦ä¹ ç‡ |
| `ICM_BETA` | 0.2 | å‰å‘/é€†å‘æŸå¤±å¹³è¡¡ |
| `ICM_ETA` | 0.1 | å†…åœ¨å¥–åŠ±ç¼©æ”¾ |
| `ICM_FEATURE_DIM` | 128 | ç¼–ç ç‰¹å¾ç»´åº¦ |

#### å¥–åŠ±ç»“æ„
| å¥–åŠ±ç±»å‹ | å€¼ | æè¿° |
|----------|----|------|
| `REWARD_FOOD` | +10.0 | åƒåˆ°é£Ÿç‰© |
| `REWARD_DEATH` | -10.0 | æ’å¢™/èº«ä½“ |
| `REWARD_STEP` | 0.0 | æ¯æ­¥æƒ©ç½š |
| `REWARD_SURVIVAL` | +0.1 | ç”Ÿå­˜å¥–åŠ± |

###  æ–‡ä»¶ç»“æ„

```
snake_ppo/
â”œâ”€â”€ runs/                   # è®­ç»ƒè¾“å‡º
â”‚   â”œâ”€â”€ [æ—¶é—´æˆ³]/           # å•æ¬¡è®­ç»ƒ
â”‚   â”‚   â”œâ”€â”€ events.out.tfevents  # TensorBoardæ—¥å¿—
â”‚   â”‚   â”œâ”€â”€ log.csv         # è®­ç»ƒæŒ‡æ ‡
â”‚   â”‚   â”œâ”€â”€ best.pt         # æœ€ä½³æ¨¡å‹
â”‚   â”‚   â””â”€â”€ ckpt_[ep].pt    # å®šæœŸæ£€æŸ¥ç‚¹
â””â”€â”€ snake_ppo.py        # ä¸»è®­ç»ƒè„šæœ¬
```

###  æ€§èƒ½æŒ‡æ ‡

è®­ç»ƒè¿‡ç¨‹ä¸­è·Ÿè¸ªä»¥ä¸‹æŒ‡æ ‡:

1. **å›åˆå¥–åŠ±**: æ¯å›åˆæ€»å¥–åŠ±
2. **å¾—åˆ†**: è›‡é•¿åº¦ - 1 (åƒåˆ°é£Ÿç‰©æ•°)
3. **å›åˆé•¿åº¦**: æ­¥æ•°æŒç»­æ—¶é—´
4. **æŸå¤±ç»„ä»¶**:
   - ActoræŸå¤±(ç­–ç•¥æ¢¯åº¦)
   - CriticæŸå¤±(ä»·å€¼ä¼°è®¡)
   - ç†µ(æ¢ç´¢å¥–åŠ±)
   - ICMæŸå¤±(å¥½å¥‡å¿ƒæ¨¡å—)
5. **å†…åœ¨å¥–åŠ±**: å¹³å‡å¥½å¥‡å¿ƒå¥–åŠ±

###  é¢„æœŸç»“æœ

| è®­ç»ƒé˜¶æ®µ | é¢„æœŸå¾—åˆ† | è¯´æ˜ |
|----------|----------|------|
| æ—©æœŸ(1-1kè½®) | 0-5 | éšæœºæ¢ç´¢ |
| ä¸­æœŸ(1k-10kè½®) | 5-15 | å­¦ä¹ åŸºæœ¬æ¨¡å¼ |
| åæœŸ(10k+è½®) | 15-30+ | ç­–ç•¥æ€§ç©æ³• |

### ğŸ›  è‡ªå®šä¹‰æŒ‡å—

#### ç¯å¢ƒä¿®æ”¹
```python
# åœ¨Cfgç±»ä¸­:
GRID_W = 15  # ä¿®æ”¹ç½‘æ ¼å®½åº¦
GRID_H = 15  # ä¿®æ”¹ç½‘æ ¼é«˜åº¦
CELL_SIZE = 30  # ä¿®æ”¹æ¸²æŸ“å°ºå¯¸
```

#### å¥–åŠ±è°ƒæ•´
```python
REWARD_FOOD = 15.0  # å¢åŠ é£Ÿç‰©å¥–åŠ±
REWARD_DEATH = -15.0 # åŠ å¼ºæ­»äº¡æƒ©ç½š
REWARD_SURVIVAL = 0.05 # å‡å°‘ç”Ÿå­˜å¥–åŠ±
```

#### ç½‘ç»œæ¶æ„
```python
HIDDEN_DIM = 512  # æ›´å°çš„ç½‘ç»œ
# æˆ–
HIDDEN_DIM = 1024 # æ›´å¤§çš„ç½‘ç»œ
```

#### è®­ç»ƒå‚æ•°
```python
BATCH_SIZE = 4096  # æ›´å¤§çš„æ‰¹é‡
LR = 3e-4         # æ›´å¿«çš„å­¦ä¹ 
GAMMA = 0.99      # æ›´çŸ­æœŸçš„å…³æ³¨
```


This enhanced README provides:

1. **Detailed technical specifications** of all components
2. **Comprehensive configuration guides** with tables
3. **Troubleshooting section** for common issues
4. **Performance expectations** at different stages
5. **Visual structure** with emojis and clear sections
6. **Deep customization instructions** for all aspects
7. **Bilingual support** with parallel content

The document is organized to help users:
- Quickly understand the project
- Easily set up and run the code
- Modify parameters effectively
- Diagnose and solve problems
- Monitor and interpret results
