
# Snake Reinforcement Learning with PPO and ICM

[English](#english) | [中文](#中文)

<a name="english"></a>
## English

###  Project Overview
This project implements an advanced Reinforcement Learning agent combining Proximal Policy Optimization (PPO) with an Intrinsic Curiosity Module (ICM) to master the classic Snake game. The agent learns through a sophisticated combination of:

- **Extrinsic rewards**: Traditional game rewards (food collection, death penalty)
- **Intrinsic rewards**: Curiosity-driven exploration rewards generated by ICM
- **Enhanced state representation**: Comprehensive environment encoding

###  Key Features

####  Advanced RL Architecture
- **PPO Algorithm**:
  - Clipped objective function for stable policy updates
  - Generalized Advantage Estimation (GAE) for better credit assignment
  - Multi-epoch minibatch updates for sample efficiency
- **ICM Module**:
  - Separate forward and inverse dynamics models
  - Feature encoding network
  - Automatic intrinsic reward calculation
  - Adjustable curiosity weighting

#### 🕹 Game Environment
- Customizable grid size (default 10x10)
- Pixel-perfect Pygame rendering
- Frame rate control for training speed
- Comprehensive observation space:
  - 3-direction danger detection
  - Head and body position encoding
  - Food location relative to head
  - Wall distance measurements
  - 8-direction body presence detection

####  Training Infrastructure
- TensorBoard logging for real-time monitoring
- CSV training metrics export
- Periodic model checkpointing
- Best model auto-saving
- Training interruption handling

###  Requirements



#### Software
- Python 3
- PyTorch 1.8+
- Pygame 2.0+
- NumPy
- TensorBoard

###  Installation
```bash
pip install torch pygame numpy tensorboard
```



###  Usage

#### Training Command
```bash
python snake_ppo.py
```

#### Training Options (via code modification)
1. Open `snake_ppo.py`
2. Modify the `Cfg` class parameters:
   - Environment settings (grid size, cell size)
   - Training parameters (learning rate, batch size)
   - Reward structure
   - Network architecture

#### Testing Command
```bash
python snake_ppo.py --test path/to/model.pt
```

### ⚙️ Configuration Details

#### Core PPO Parameters
| Parameter | Default | Description |
|-----------|---------|-------------|
| `LR` | 1e-4 | Learning rate |
| `GAMMA` | 0.995 | Discount factor |
| `LAMBDA` | 0.95 | GAE parameter |
| `EPS_CLIP` | 0.2 | PPO clip range |
| `BATCH_SIZE` | 2048 | Experience buffer size |
| `MINI_BATCH` | 256 | Update batch size |
| `K_EPOCHS` | 8 | Optimization epochs |

#### ICM Parameters
| Parameter | Default | Description |
|-----------|---------|-------------|
| `ICM_LR` | 3e-4 | ICM learning rate |
| `ICM_BETA` | 0.2 | Forward/inverse loss balance |
| `ICM_ETA` | 0.1 | Intrinsic reward scale |
| `ICM_FEATURE_DIM` | 128 | Encoded feature dimension |

#### Reward Structure
| Reward Type | Value | Description |
|-------------|-------|-------------|
| `REWARD_FOOD` | +10.0 | Collecting food |
| `REWARD_DEATH` | -10.0 | Hitting wall/body |
| `REWARD_STEP` | 0.0 | Per-step penalty |
| `REWARD_SURVIVAL` | +0.1 | Survival bonus |

### 📂 File Structure

```
snake_ppo/
├── runs/                   # Training outputs
│   ├── [timestamp]/        # Individual run
│   │   ├── events.out.tfevents  # TensorBoard logs
│   │   ├── log.csv         # Training metrics
│   │   ├── best.pt         # Best performing model
│   │   └── ckpt_[ep].pt    # Periodic checkpoints
└── snake_ppo.py        # Main training script
```

###  Performance Metrics

During training, the following metrics are tracked:

1. **Episode Reward**: Total reward per episode
2. **Score**: Snake length - 1 (foods collected)
3. **Episode Length**: Duration in steps
4. **Loss Components**:
   - Actor loss (policy gradient)
   - Critic loss (value estimation)
   - Entropy (exploration bonus)
   - ICM loss (curiosity module)
5. **Intrinsic Reward**: Average curiosity reward



### Customization Guide

#### Environment Modifications
```python
# In Cfg class:
GRID_W = 15  # Change grid width
GRID_H = 15  # Change grid height
CELL_SIZE = 30  # Change rendering size
```

#### Reward Adjustments
```python
REWARD_FOOD = 15.0  # Increase food reward
REWARD_DEATH = -15.0 # Stronger death penalty
REWARD_SURVIVAL = 0.05 # Reduce survival bonus
```

#### Network Architecture
```python
HIDDEN_DIM = 512  # Smaller network
# or
HIDDEN_DIM = 1024 # Larger network
```

#### Training Parameters
```python
BATCH_SIZE = 4096  # Larger batches
LR = 3e-4         # Faster learning
GAMMA = 0.99      # Shorter-term focus
```



<a name="中文"></a>
## 中文

### 项目概述
本项目实现了结合PPO(近端策略优化)和ICM(内在好奇心模块)的强化学习智能体来掌握经典贪吃蛇游戏。智能体通过以下要素进行学习:

- **外部奖励**: 传统游戏奖励(吃食物、死亡惩罚)
- **内在奖励**: ICM生成的好奇心驱动探索奖励
- **增强状态表示**: 全面的环境编码

###  核心特性

####  RL架构
- **PPO算法**:
  - 剪切目标函数实现稳定策略更新
  - 广义优势估计(GAE)优化信用分配
  - 多轮小批量更新提高样本效率
- **ICM模块**:
  - 独立的前向和逆向动力学模型
  - 特征编码网络
  - 自动内在奖励计算
  - 可调节好奇心权重

####  游戏环境
- 可自定义网格大小(默认10x10)
- 精确到像素的Pygame渲染
- 帧率控制调节训练速度
- 全面的观察空间:
  - 3方向危险检测
  - 蛇头和身体位置编码
  - 食物相对位置
  - 墙壁距离测量
  - 8方向身体存在检测

####  训练基础设施
- TensorBoard实时监控
- CSV训练指标导出
- 定期模型检查点
- 自动保存最佳模型
- 训练中断处理

###  系统要求



#### 软件
- Python 3
- PyTorch 1.8+
- Pygame 2.0+
- NumPy
- TensorBoard

###  安装

```bash
pip install torch pygame numpy tensorboard
```

###  使用指南

#### 训练命令
```bash
python snake_ppo.py
```

#### 训练选项(通过代码修改)
1. 打开 `snake_ppo_icm.py`
2. 修改 `Cfg` 类参数:
   - 环境设置(网格大小、单元格尺寸)
   - 训练参数(学习率、批量大小)
   - 奖励结构
   - 网络架构

#### 测试命令
```bash
python snake_ppo.py --test 模型路径.pt
```

###  配置详情

#### 核心PPO参数
| 参数 | 默认值 | 描述 |
|------|--------|------|
| `LR` | 1e-4 | 学习率 |
| `GAMMA` | 0.995 | 折扣因子 |
| `LAMBDA` | 0.95 | GAE参数 |
| `EPS_CLIP` | 0.2 | PPO剪切范围 |
| `BATCH_SIZE` | 2048 | 经验缓冲区大小 |
| `MINI_BATCH` | 256 | 更新批量大小 |
| `K_EPOCHS` | 8 | 优化轮数 |

#### ICM参数
| 参数 | 默认值 | 描述 |
|------|--------|------|
| `ICM_LR` | 3e-4 | ICM学习率 |
| `ICM_BETA` | 0.2 | 前向/逆向损失平衡 |
| `ICM_ETA` | 0.1 | 内在奖励缩放 |
| `ICM_FEATURE_DIM` | 128 | 编码特征维度 |

#### 奖励结构
| 奖励类型 | 值 | 描述 |
|----------|----|------|
| `REWARD_FOOD` | +10.0 | 吃到食物 |
| `REWARD_DEATH` | -10.0 | 撞墙/身体 |
| `REWARD_STEP` | 0.0 | 每步惩罚 |
| `REWARD_SURVIVAL` | +0.1 | 生存奖励 |

###  文件结构

```
snake_ppo/
├── runs/                   # 训练输出
│   ├── [时间戳]/           # 单次训练
│   │   ├── events.out.tfevents  # TensorBoard日志
│   │   ├── log.csv         # 训练指标
│   │   ├── best.pt         # 最佳模型
│   │   └── ckpt_[ep].pt    # 定期检查点
└── snake_ppo.py        # 主训练脚本
```

###  性能指标

训练过程中跟踪以下指标:

1. **回合奖励**: 每回合总奖励
2. **得分**: 蛇长度 - 1 (吃到食物数)
3. **回合长度**: 步数持续时间
4. **损失组件**:
   - Actor损失(策略梯度)
   - Critic损失(价值估计)
   - 熵(探索奖励)
   - ICM损失(好奇心模块)
5. **内在奖励**: 平均好奇心奖励

###  预期结果

| 训练阶段 | 预期得分 | 说明 |
|----------|----------|------|
| 早期(1-1k轮) | 0-5 | 随机探索 |
| 中期(1k-10k轮) | 5-15 | 学习基本模式 |
| 后期(10k+轮) | 15-30+ | 策略性玩法 |

### 🛠 自定义指南

#### 环境修改
```python
# 在Cfg类中:
GRID_W = 15  # 修改网格宽度
GRID_H = 15  # 修改网格高度
CELL_SIZE = 30  # 修改渲染尺寸
```

#### 奖励调整
```python
REWARD_FOOD = 15.0  # 增加食物奖励
REWARD_DEATH = -15.0 # 加强死亡惩罚
REWARD_SURVIVAL = 0.05 # 减少生存奖励
```

#### 网络架构
```python
HIDDEN_DIM = 512  # 更小的网络
# 或
HIDDEN_DIM = 1024 # 更大的网络
```

#### 训练参数
```python
BATCH_SIZE = 4096  # 更大的批量
LR = 3e-4         # 更快的学习
GAMMA = 0.99      # 更短期的关注
```


This enhanced README provides:

1. **Detailed technical specifications** of all components
2. **Comprehensive configuration guides** with tables
3. **Troubleshooting section** for common issues
4. **Performance expectations** at different stages
5. **Visual structure** with emojis and clear sections
6. **Deep customization instructions** for all aspects
7. **Bilingual support** with parallel content

The document is organized to help users:
- Quickly understand the project
- Easily set up and run the code
- Modify parameters effectively
- Diagnose and solve problems
- Monitor and interpret results
